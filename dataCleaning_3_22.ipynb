{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "5751d7c496c272386542b26abd80dd065779a39feb74bf22f2b4e5e7f629ef29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### General Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from cleanUp import cleanUp\n",
    "from fillDf import fillDf\n",
    "from fixYearStamp import fixYearStamp\n",
    "from sklearn.cluster import KMeans\n",
    "import time as clock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = clock.time()"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = glob.glob(\"./Data/*.txt\")\n",
    "# insert the desired start time\n",
    "cutOffTime = '3/22/2021 9:30'\n",
    "endTime = '3/22/2021 13:00'\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "columns = [0,1,6,7,8,9,10,11]\n",
    "# Enable Data Checking\n",
    "DataChecking = False\n",
    "# Here are obversed timestamps that need to removed from the data\n",
    "badTimes = ['     0/0/0      0:0:0','2165/165/165 165:165:85']\n",
    "# Controls wether zones will be created automatically or by k-means clusters\n",
    "ZoneAutomation = False\n",
    "# Sets either the binning or the manual zones\n",
    "numberOfZones = 4\n",
    "numAutoZones = 3\n",
    "# Sensors to exclude from zone\n",
    "outdoorSensors = ['S-16','S-17','S-18','S-19']\n",
    "# 10s of seconds before nebulization to include in the expirement csv files\n",
    "preCursorFactor = 6\n",
    "# which particle to analyze\n",
    "particle = 'Dp>0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expTRange = {\n",
    "\n",
    "    'EE502 Door Closed':\n",
    "    [#pd.Timestamp('3/22/2021 9:40'), # removed this one due to odd behavior\n",
    "    pd.Timestamp('3/22/2021 10:05:23'),\n",
    "    pd.Timestamp('3/22/2021 10:23:52')],\n",
    "    'EE502 Door Open':\n",
    "    [pd.Timestamp('3/22/2021 10:42:43'),\n",
    "    pd.Timestamp('3/22/2021 10:59:23'),\n",
    "    pd.Timestamp('3/22/2021 11:15:37')],\n",
    "    'EE502 Negative Pressure':\n",
    "    [pd.Timestamp('3/22/2021 11:32:21'),\n",
    "    pd.Timestamp('3/22/2021 11:42:27'),\n",
    "    pd.Timestamp('3/22/2021 11:53:47')],\n",
    "    'EE504 Door Open':\n",
    "    [pd.Timestamp('3/22/2021 12:19:12'),\n",
    "    pd.Timestamp('3/22/2021 12:30:10'),\n",
    "    pd.Timestamp('3/22/2021 12:40:15')],\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'EE502 Door Closed' : 15*6,\n",
    "    'EE502 Door Open':15*6,\n",
    "    'EE502 Negative Pressure':10*6,\n",
    "    'EE504 Door Open':10*6,\n",
    "}\n",
    "\n",
    "# Manual Zone set up notice how we are missing S-14\n",
    "zoneList = {\n",
    "    'Zone 1' : ['S-01','S-04'],\n",
    "    'Zone 2' : ['S-02','S-03','S-05','S-06'],\n",
    "    'Zone 3' : ['S-07','S-08','S-09','S-10','S-11','S-12','S-13','S-15','S-16'],\n",
    "    'Zone 4' : ['S-17','S-18'],\n",
    "    'Zone 5' : ['S-19']\n",
    "}\n",
    "if not ZoneAutomation:\n",
    "    numberOfZones = len(zoneList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./Data\\\\S-01.txt',\n",
       " './Data\\\\S-02.txt',\n",
       " './Data\\\\S-03.txt',\n",
       " './Data\\\\S-04.txt',\n",
       " './Data\\\\S-05.txt',\n",
       " './Data\\\\S-06.txt',\n",
       " './Data\\\\S-07.txt',\n",
       " './Data\\\\S-08.txt',\n",
       " './Data\\\\S-09.txt',\n",
       " './Data\\\\S-10.txt',\n",
       " './Data\\\\S-11.txt',\n",
       " './Data\\\\S-12.txt',\n",
       " './Data\\\\S-13.txt',\n",
       " './Data\\\\S-15.txt',\n",
       " './Data\\\\S-16.txt',\n",
       " './Data\\\\S-17.txt',\n",
       " './Data\\\\S-18.txt',\n",
       " './Data\\\\S-19.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "all_csv_files"
   ]
  },
  {
   "source": [
    "Changed this to markdown so it won't run twice, had to fix the timestamps on S-12\n",
    "filePath        = all_csv_files[11]\n",
    "incorrectString = '21/3/22'\n",
    "date            = '3/22/2021'\n",
    "charTimeStart   = 11\n",
    "charTimeEnd     = 21\n",
    "offset          = 0\n",
    "fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01     2021-03-22 09:30:00      2021-03-22 13:00:19       mod: yes\nS-02     2021-03-22 09:30:01      2021-03-22 12:59:21       mod: yes\nS-03     2021-03-22 09:30:00      2021-03-22 12:59:09       mod: yes\nS-04     2021-03-22 09:30:00      2021-03-22 12:59:00       mod: yes\nS-05     2021-03-22 09:30:00      2021-03-22 12:59:29       mod: yes\nS-06     2021-03-22 09:30:00      2021-03-22 13:00:49       mod: yes\nS-07     2021-03-22 09:30:05      2021-03-22 12:58:50       mod: no\nS-08     2021-03-22 09:30:05      2021-03-22 12:58:46       mod: no\nS-09     2021-03-22 09:30:16      2021-03-22 12:59:26       mod: no\nS-10     2021-03-22 09:30:06      2021-03-22 12:59:32       mod: no\nS-11     2021-03-22 09:30:03      2021-03-22 12:59:53       mod: no\nS-12     2021-03-22 09:30:08      2021-03-22 12:59:30       mod: no\nS-13     2021-03-22 09:30:05      2021-03-22 12:59:15       mod: no\nS-15     2021-03-22 09:30:00      2021-03-22 12:59:03       mod: yes\nS-16     2021-03-22 09:30:00      2021-03-22 13:32:35       mod: no\nS-17     2021-03-22 09:30:00      2021-03-22 13:26:45       mod: no\nS-18     2021-03-22 11:23:18      2021-03-22 13:01:22       mod: no\nS-19     2021-03-22 09:30:07      2021-03-22 13:01:21       mod: yes\n"
     ]
    }
   ],
   "source": [
    "data = cleanUp(cutOffTime,sensorConditions,all_csv_files,columns,badTimes)"
   ]
  },
  {
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Checking Data\n",
    "Here we scan through the data for irregularities in data recording."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DataChecking:\n",
    "    directory = './dataInfo'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fout = open('./dataInfo/time_Frequency_Error_Log.txt','wt')\n",
    "    errors = {}\n",
    "    errorCount = {}\n",
    "    # Enter the expected interval here\n",
    "    interval = 10\n",
    "    for x in data:\n",
    "        # errors keeps track of length of each time interval error that occurs\n",
    "        errors[x] = set(())\n",
    "        # errorCount keeps track of how many times each time interval error occured\n",
    "        errorCount[x] = {}\n",
    "        # counter keeps track of the total time interval errors per sensor\n",
    "        counter = 0\n",
    "        #shows the total\n",
    "        temp = data[x]\n",
    "        for idx,i in enumerate(temp['Date_Time']):\n",
    "            try:\n",
    "                if not ((temp['Date_Time'][idx+1] - i) == pd.Timedelta(seconds=interval)):\n",
    "                    timeErr = temp['Date_Time'][idx+1] - i\n",
    "                    if str(timeErr.seconds) in errorCount[x]:\n",
    "                        errorCount[x][str(timeErr.seconds)] +=1\n",
    "                    else:\n",
    "                        errorCount[x][str(timeErr.seconds)] = 1\n",
    "\n",
    "                    errors[x].add(timeErr)\n",
    "\n",
    "\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(str(round(counter/len(temp)*100,2)),'% potential error in ', x)\n",
    "        fout.write('potential error in '+ x +'\\n' + str(round(counter/len(temp)*100,2))+'%'+'\\n')\n",
    "\n",
    "        # display the different types of errors\n",
    "        lst = [i.seconds for i in errors[x]]\n",
    "        frmt = \"{:>4}\"*len(lst)\n",
    "        print(frmt.format(*lst))\n",
    "        fout.write(\"Time Errors\" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "        # display the quantity of each type of error\n",
    "        lst = [errorCount[x][str(i.seconds)] for i in errors[x]]\n",
    "        frmt = \"{:>4}\"*len(lst)\n",
    "        print(frmt.format(*lst))\n",
    "        fout.write(\"# Observed \" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "        print()\n",
    "        fout.write('\\n')\n",
    "\n",
    "\n",
    "    fout.close()"
   ]
  },
  {
   "source": [
    "Notice there are quite a few repeating errors here in our data set. We can either choose to interpolate the data inbetween or pad it with 0s. For gaps <40s i will interpolate, but for gaps >40 i will 0 pad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-02   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-03   ['% of values from interpolation : 0.159', '% of values from 0-padding : 0.0', '% of values not changed : 99.841']\n",
      "S-04   ['% of values from interpolation : 0.398', '% of values from 0-padding : 0.0', '% of values not changed : 99.602']\n",
      "S-05   ['% of values from interpolation : 0.477', '% of values from 0-padding : 6.762', '% of values not changed : 92.761']\n",
      "S-06   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-07   ['% of values from interpolation : 33.413', '% of values from 0-padding : 0.0', '% of values not changed : 66.587']\n",
      "S-08   ['% of values from interpolation : 33.36', '% of values from 0-padding : 0.0', '% of values not changed : 66.64']\n",
      "S-09   ['% of values from interpolation : 66.667', '% of values from 0-padding : 0.0', '% of values not changed : 33.333']\n",
      "S-10   ['% of values from interpolation : 36.407', '% of values from 0-padding : 1.033', '% of values not changed : 62.56']\n",
      "S-11   ['% of values from interpolation : 33.333', '% of values from 0-padding : 0.0', '% of values not changed : 66.667']\n",
      "S-12   ['% of values from interpolation : 89.348', '% of values from 0-padding : 10.413', '% of values not changed : 0.238']\n",
      "S-13   ['% of values from interpolation : 33.28', '% of values from 0-padding : 0.0', '% of values not changed : 66.72']\n",
      "S-15   ['% of values from interpolation : 0.637', '% of values from 0-padding : 0.398', '% of values not changed : 98.964']\n",
      "S-16   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.556', '% of values not changed : 99.444']\n",
      "S-17   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-18   ['% of values from interpolation : 0.159', '% of values from 0-padding : 53.968', '% of values not changed : 45.873']\n",
      "S-19   ['% of values from interpolation : 0.159', '% of values from 0-padding : 0.0', '% of values not changed : 99.841']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,cutOffTime,endTime,cutoff)\n",
    "        print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "source": [
    "### Export Data\n",
    "export the newly interpolated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './interpolatedData'\n",
    "for x in interpDF:\n",
    "    temp=interpDF[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Merge the DataFrames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 1253\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "lowIDX,lowValue = [[i,value] for i,value in enumerate(length) if value == index][0]\n",
    "print(lowIDX,lowValue)"
   ]
  },
  {
   "source": [
    "for count,key in enumerate(list(interpDF.keys())):\n",
    "    print(count+1,key,temp[count+1])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date_Time  S-01  S-02  S-03  S-04  S-05  S-06  S-07  S-08  \\\n",
       "0    2021-03-22 09:30:00     0     0     0     0     0    21     9     0   \n",
       "1    2021-03-22 09:30:10     0     0     0     0     0    30     0     0   \n",
       "2    2021-03-22 09:30:20     0     0     0     0     0    21     0     0   \n",
       "3    2021-03-22 09:30:30    42     9     0     0     0     0     0     0   \n",
       "4    2021-03-22 09:30:40     0     9     0     0     0     0     0     0   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1248 2021-03-22 12:58:00     9     0     0     0     0     0     0     9   \n",
       "1249 2021-03-22 12:58:10     0     0     0     0     0     0     0     9   \n",
       "1250 2021-03-22 12:58:20     0     9     0     0     0     0     0     0   \n",
       "1251 2021-03-22 12:58:30     0     9     0     0     0     0     0     0   \n",
       "1252 2021-03-22 12:58:40     0     0     9     0     0     0     0     4   \n",
       "\n",
       "      S-09  ...  S-11  S-12  S-13  S-15  S-16  S-17  S-18  S-19    Average  \\\n",
       "0      702  ...     0     0     0     0     0     0     0     0  48.800000   \n",
       "1      702  ...     0     0     0     0     0     9     0     0  48.800000   \n",
       "2       99  ...     0     0     0     0     0     9     0     0   8.000000   \n",
       "3       99  ...     0     0     0     0    45     0     0     0  13.000000   \n",
       "4       49  ...    21    30     0     0     0     0     0     4   7.266667   \n",
       "...    ...  ...   ...   ...   ...   ...   ...   ...   ...   ...        ...   \n",
       "1248     0  ...     0     0     0     0     0    30     0     0   1.200000   \n",
       "1249     0  ...     0     0     0     0     0    30     0     0   0.600000   \n",
       "1250     0  ...     0     0     0     0     0     0     0     0   0.600000   \n",
       "1251     0  ...     0     0     0     0     0     0     0     0   0.600000   \n",
       "1252     0  ...     0     0     0     0     0     0     0     0   0.866667   \n",
       "\n",
       "          Variance  \n",
       "0     30506.960000  \n",
       "1     30532.160000  \n",
       "2       618.800000  \n",
       "3       742.400000  \n",
       "4       202.062222  \n",
       "...            ...  \n",
       "1248      9.360000  \n",
       "1249      5.040000  \n",
       "1250      5.040000  \n",
       "1251      5.040000  \n",
       "1252      5.715556  \n",
       "\n",
       "[1253 rows x 21 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date_Time</th>\n      <th>S-01</th>\n      <th>S-02</th>\n      <th>S-03</th>\n      <th>S-04</th>\n      <th>S-05</th>\n      <th>S-06</th>\n      <th>S-07</th>\n      <th>S-08</th>\n      <th>S-09</th>\n      <th>...</th>\n      <th>S-11</th>\n      <th>S-12</th>\n      <th>S-13</th>\n      <th>S-15</th>\n      <th>S-16</th>\n      <th>S-17</th>\n      <th>S-18</th>\n      <th>S-19</th>\n      <th>Average</th>\n      <th>Variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-03-22 09:30:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>9</td>\n      <td>0</td>\n      <td>702</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>48.800000</td>\n      <td>30506.960000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-03-22 09:30:10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>702</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>48.800000</td>\n      <td>30532.160000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-03-22 09:30:20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>99</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.000000</td>\n      <td>618.800000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-03-22 09:30:30</td>\n      <td>42</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>99</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.000000</td>\n      <td>742.400000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-03-22 09:30:40</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>49</td>\n      <td>...</td>\n      <td>21</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7.266667</td>\n      <td>202.062222</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1248</th>\n      <td>2021-03-22 12:58:00</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.200000</td>\n      <td>9.360000</td>\n    </tr>\n    <tr>\n      <th>1249</th>\n      <td>2021-03-22 12:58:10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.600000</td>\n      <td>5.040000</td>\n    </tr>\n    <tr>\n      <th>1250</th>\n      <td>2021-03-22 12:58:20</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.600000</td>\n      <td>5.040000</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>2021-03-22 12:58:30</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.600000</td>\n      <td>5.040000</td>\n    </tr>\n    <tr>\n      <th>1252</th>\n      <td>2021-03-22 12:58:40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.866667</td>\n      <td>5.715556</td>\n    </tr>\n  </tbody>\n</table>\n<p>1253 rows Ã— 21 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "columns = list(interpDF.keys())\n",
    "mergedData = pd.DataFrame({'Date_Time':interpDF[columns[lowIDX]]['Date_Time']})\n",
    "for idx,column in enumerate(columns):\n",
    "    mergedData[column] = interpDF[column][particle]\n",
    "Average = np.mean(mergedData[zoneList['Zone 1']+zoneList['Zone 2']+zoneList['Zone 3']],axis=1)\n",
    "Variance = np.var(mergedData[zoneList['Zone 1']+zoneList['Zone 2']+zoneList['Zone 3']],axis=1)\n",
    "mergedData['Average'] = Average\n",
    "mergedData['Variance'] = Variance\n",
    "mergedData"
   ]
  },
  {
   "source": [
    "### Increase Resolution on mergedData"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())"
   ]
  },
  {
   "source": [
    "### Export Merged Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Create csv files for each animation\n",
    "We have 3 expirements in each that we want to average across the range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "source": [
    "## Determining Zones\n",
    "Here we first create 'averagedFrame's. These are dictionaries that at each 'label' (which corresponds to the name of an expirement) we have a pandas dataframe containing the results of all of the trails in an expirement summed, and then divided by the total number of trails.\n",
    "Anytime you are adjusting the Zones, everything below here must be run. The values of many of these DataFrames are mutated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'EE502 Door Closed': [213, 324],\n",
       " 'EE502 Door Open': [437, 537, 634],\n",
       " 'EE502 Negative Pressure': [735, 795, 863],\n",
       " 'EE504 Door Open': [1016, 1081, 1142]}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "expIndexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preCursorFactor is defined at the start\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "    runSumFrames = expIndexes[label][0]-expIndexes[label][0]\n",
    "    for idx,time in enumerate(expIndexes[label]):\n",
    "        start = expIndexes[label][idx] - preCursorFactor\n",
    "        end = expIndexes[label][idx] + expTLen[label]\n",
    "        expirementFrame[label+' Exp '+str(idx+1)] = mergedData.iloc[ start : end , 1: ].reset_index(drop = True)\n",
    "        runSumFrames += expirementFrame[label+' Exp '+str(idx+1)]\n",
    "        \n",
    "    averagedFrame[label] = runSumFrames/(idx+1)"
   ]
  },
  {
   "source": [
    "Calculating the correct Zones for each expirement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# numAutoZones is defined at the start\n",
    "AutoZoneAssignments = {}\n",
    "for frame in averagedFrame:\n",
    "    # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "    avgFrm = averagedFrame[frame]\n",
    "    # outdoorSensors must have its spelling exactly match\n",
    "    columns = list(set(avgFrm.keys()[:-2])- set(outdoorSensors))\n",
    "    columns.sort()\n",
    "\n",
    "    X = {}\n",
    "    for column in columns:\n",
    "        value,index = max([(value,index) for index,value in enumerate(avgFrm[column])]) \n",
    "        X[column] = np.array([np.log(value+.01),index])\n",
    "    X = [X[i] for i in X]\n",
    "    kmeans = KMeans(n_clusters=numAutoZones,random_state=0).fit(X)\n",
    "    idx = np.argsort(kmeans.cluster_centers_.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(numAutoZones)\n",
    "    #lut = lut[::-1]\n",
    "    orderedZones = [[]]*numAutoZones\n",
    "    for index, zone in enumerate(lut):\n",
    "        orderedZones[index] = [index if zone == kmeans.labels_[i] else 0 for i in range(len(kmeans.labels_))]\n",
    "    AutoZoneAssignments[frame] = np.sum(orderedZones,axis=0)\n",
    "z = numAutoZones\n",
    "ZDfAuto = pd.DataFrame(AutoZoneAssignments)\n",
    "ZDfAuto = ZDfAuto.append(pd.DataFrame([[z]*len(expIndexes)]*len(outdoorSensors),columns = AutoZoneAssignments.keys()),ignore_index=True)\n",
    "AutoZoneAssignments = ZDfAuto\n",
    "if len(outdoorSensors):\n",
    "    numAutoZones += 1\n",
    "\n",
    "if not ZoneAutomation:\n",
    "    ZoneAssignments = {}\n",
    "    for frame in averagedFrame:\n",
    "        # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "        avgFrm = averagedFrame[frame]\n",
    "        # outdoorSensors must have its spelling exactly match\n",
    "        columns = list(set(avgFrm.keys()[:-2]))\n",
    "        columns.sort()\n",
    "        ZoneAssignments[frame] = [0]*len(columns)\n",
    "        for value,zone in enumerate(zoneList):\n",
    "            for sensor in zoneList[zone]:\n",
    "                ZoneAssignments[frame][columns.index(sensor)] = value\n",
    "    ZDf = pd.DataFrame(ZoneAssignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'ZoneAssignments.csv')\n",
    "ZDf.to_csv(location,index=False)\n",
    "\n",
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'AutoZoneAssignments.csv')\n",
    "ZDfAuto.to_csv(location,index=False)\n",
    "\n",
    "import copy\n",
    "expirementFrameAuto = copy.deepcopy(expirementFrame)\n",
    "averagedFrameAuto = copy.deepcopy(averagedFrame)"
   ]
  },
  {
   "source": [
    "Zoning the expirement data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Zoning the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonedAvgFrame = {}\n",
    "for key in ZoneAssignments:\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    zonedAvgFrame[key] = averagedFrame[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedAvgFrame[key][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedAvgFrame[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "\n",
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrame = {}\n",
    "# create a list of all of the various dict keys in expirementFrame so that we can iterate through them to get the data\n",
    "labels = list(expirementFrame.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrame[exp] = expirementFrame[exp]\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedExpFrame[exp][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedExpFrame[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "\n",
    "        \n",
    "zonedAvgFrameAuto = {}\n",
    "for key in AutoZoneAssignments:\n",
    "    occourances = [list(AutoZoneAssignments[key]).count(x) for x in set(AutoZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numAutoZones\n",
    "    zonedAvgFrameAuto[key] = averagedFrameAuto[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[AutoZoneAssignments[key][idx]] += zonedAvgFrameAuto[key][column]\n",
    "    for idx in range(numAutoZones):\n",
    "        zonedAvgFrameAuto[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "        \n",
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrameAuto = {}\n",
    "# create a list of all of the various dict keys in expirementFrameAuto so that we can iterate through them to get the data\n",
    "labels = list(expirementFrameAuto.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp ')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numAutoZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrameAuto[exp] = expirementFrameAuto[exp]\n",
    "    occourances = [list(AutoZoneAssignments[key]).count(x) for x in set(AutoZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[AutoZoneAssignments[key][idx]] += zonedExpFrameAuto[exp][column]\n",
    "    for idx in range(numAutoZones):\n",
    "        zonedExpFrameAuto[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './averagedData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrame:\n",
    "    temp=averagedFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './averagedDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrameAuto:\n",
    "    temp=averagedFrameAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './expirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './expirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Increase the Resolution\n",
    "pad out the dataframes to have values for every second."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)\n",
    "\n",
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns) \n",
    "    \n",
    "stretchedDFAuto = {}\n",
    "for i in averagedFrameAuto:\n",
    "    tempFrame = averagedFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDFAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns) \n",
    "\n",
    "stretchExpDfAuto = {}\n",
    "for i in expirementFrameAuto:\n",
    "    tempFrame = expirementFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDfAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedAvgDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDFAuto:\n",
    "    temp=stretchedDFAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDfAuto:\n",
    "    temp=stretchExpDfAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23.71465563774109\n"
     ]
    }
   ],
   "source": [
    "end = clock.time()\n",
    "print(end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}